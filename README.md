# CMSI 402

## Human Motion Initiated Music Generation

### Overview
The objective of this project is to enable a single user to create music through movement. The user’s movement will be tracked by the Kinect (version 2) and mapped to sound in real time. The system will be able to automatically generate musical notes based on the changes in velocity, acceleration, and position of a set of skeletal joints in the user’s body. The intention is to have the system also support looping, partitioning the physical space for control of different instruments at once, and more. 
	
### Related Work
The ISMM Team at IRCAM, a French institute for science about music and sound, conducts research and development on interactive music systems, gesture and sound modeling, interactive music synthesis, and more. Many of the projects they have presented focus more on gesture recognition for triggering sounds rather than _dance_ movements. This project will have more of an emphasis on **whole-body non-gestural movements**.
